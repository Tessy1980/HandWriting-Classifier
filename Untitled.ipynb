{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b9d7d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b30061a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ec3e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models,layers,Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31e7b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16faed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_label),(test_images,test_label)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e9904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b452cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7763ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=train_images[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38c3639e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efcda1a48d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample,cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be518646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d33f716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images=train_images.reshape((60000,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edd697a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images=train_images.astype(\"float32\")/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c11f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e65acbe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0569637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eb3a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8071479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b51005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87a417c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6931de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images=test_images.reshape((10000,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b9d5b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images=test_images.astype(\"float32\")/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa9ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7798ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff25d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35df9619",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label= to_categorical(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f75a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label=to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8208ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=models.Sequential() # we want to build the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4af1cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Conv2D(32,(3,3),activation ='relu',input_shape=(28,28,1)))# This line helps to build the CONV2D NTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf57949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.MaxPooling2D((2, 2)))# This is the code for the max pooling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cbc0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4431484",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b83f0848",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())# we need to flatten it by converting it from 2D to 1d network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fbe5f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(64, activation='relu'))# Now we need to add classifier which is the dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a13bc2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(10, activation='softmax'))# This is the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a53eaa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbfa9bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])             #we want to compile the network . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21292d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 14s 226us/sample - loss: 0.1703 - accuracy: 0.9466\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 14s 240us/sample - loss: 0.0472 - accuracy: 0.9855\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 14s 236us/sample - loss: 0.0314 - accuracy: 0.9902\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 14s 235us/sample - loss: 0.0242 - accuracy: 0.9926\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 14s 238us/sample - loss: 0.0183 - accuracy: 0.9945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efcd84b6080>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_label, epochs=5, batch_size=64)# This accuracy is on the trained data. It would use optimiser to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da217905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 110us/sample - loss: 0.0351 - accuracy: 0.9896\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_label)#let us test on the test data to check error loss and compare with the trained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7636098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efcd71f1390>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAALFUlEQVR4nO3dT4ic9R3H8c+nVi/qIWmGEGLoWgmFUGiUIRQU2WKVmEv0IuYgKQjrQaGCh4o9ZPcWSlV6KEKswbRYpaBiDqE1DRtFKOIoaf4obaysmLBmJ+RgPNnot4d9ImvcnZnM8zzzjPm+X7DMzPPM7nwZfDt/npn8HBECcOX7XtMDABgNYgeSIHYgCWIHkiB2IInvj/LG1qxZExMTE6O8SSCVubk5nT171svtKxW77a2Sfi/pKkl/jIjdva4/MTGhTqdT5iYB9NBut1fcN/TTeNtXSfqDpLslbZK0w/amYf8egHqVec2+RdKHEfFRRHwh6SVJ26sZC0DVysS+XtInSy6fKrZ9g+0p2x3bnW63W+LmAJRR+7vxEbEnItoR0W61WnXfHIAVlIn9tKQNSy7fUGwDMIbKxP6OpI22b7R9jaT7Je2vZiwAVRv60FtEXLD9iKS/a/HQ296IOFHZZAAqVeo4e0QckHSgolkA1IiPywJJEDuQBLEDSRA7kASxA0kQO5AEsQNJEDuQBLEDSRA7kASxA0kQO5AEsQNJEDuQBLEDSRA7kASxA0kQO5AEsQNJEDuQBLEDSYx0yWaMn+np6Z77Z2Zmeu6fnJzsuX92dvYyJ0JdeGQHkiB2IAliB5IgdiAJYgeSIHYgCWIHkuA4e3JvvPFGqd8/fPjw0Pv7HaNHtUrFbntO0nlJX0q6EBHtKoYCUL0qHtl/HhFnK/g7AGrEa3YgibKxh6TXbb9re2q5K9iest2x3el2uyVvDsCwysZ+W0TcIuluSQ/bvv3SK0TEnohoR0S71WqVvDkAwyoVe0ScLk4XJL0qaUsVQwGo3tCx277W9vUXz0u6S9LxqgYDUK0y78avlfSq7Yt/5y8R8bdKpsLI9DtOXuff5zj7aA0de0R8JOmnFc4CoEYcegOSIHYgCWIHkiB2IAliB5LgK66oVb9/qhqjwyM7kASxA0kQO5AEsQNJEDuQBLEDSRA7kASxA0kQO5AEsQNJEDuQBLEDSRA7kASxA0kQO5AE32dPbteuXT33z8zMlPr7vb7PznfdR4tHdiAJYgeSIHYgCWIHkiB2IAliB5IgdiAJjrMnV/Y4Or47+j6y295re8H28SXbVts+aPtkcbqq3jEBlDXI0/jnJW29ZNvjkg5FxEZJh4rLAMZY39gj4k1J5y7ZvF3SvuL8Pkn3VDsWgKoN+wbd2oiYL85/KmntSle0PWW7Y7vT7XaHvDkAZZV+Nz4iQlL02L8nItoR0W61WmVvDsCQho39jO11klScLlQ3EoA6DBv7fkk7i/M7Jb1WzTgA6jLIobcXJf1T0o9tn7L9oKTdku60fVLSL4rLAMZY3w/VRMSOFXbdUfEsAGrEx2WBJIgdSILYgSSIHUiC2IEkiB1IgtiBJIgdSILYgSSIHUiC2IEkiB1IgtiBJIgdSILYgSSIHUiC2IEkiB1IgtiBJIgdSILYgSRYshm1mp6ebnoEFHhkB5IgdiAJYgeSIHYgCWIHkiB2IAliB5IgdiCJQdZn32t7wfbxJdumbZ+2faT42VbvmADKGuSR/XlJW5fZ/nREbC5+DlQ7FoCq9Y09It6UdG4EswCoUZnX7I/YPlo8zV+10pVsT9nu2O50u90SNwegjGFjf0bSTZI2S5qX9ORKV4yIPRHRjoh2q9Ua8uYAlDVU7BFxJiK+jIivJD0raUu1YwGo2lCx21635OK9ko6vdF0A46Hv99ltvyhpUtIa26ck7ZI0aXuzpJA0J+mh+kYEUIW+sUfEjmU2P1fDLABqxCfogCSIHUiC2IEkiB1IgtiBJIgdSILYgSSIHUiC2IEkiB1IgtiBJIgdSILYgSRYsjm5ycnJnvsPHz5c6u/3WrKZ5ZxHi0d2IAliB5IgdiAJYgeSIHYgCWIHkiB2IAliB5IgdiAJYgeSIHYgCWIHkiB2IAliB5IgdiAJvs9+hev3ffSy31fHd0ffR3bbG2zP2n7f9gnbvyq2r7Z90PbJ4nRV/eMCGNYgT+MvSHosIjZJ+pmkh21vkvS4pEMRsVHSoeIygDHVN/aImI+I94rz5yV9IGm9pO2S9hVX2yfpnppmBFCBy3qDzvaEpJslvS1pbUTMF7s+lbR2hd+Zst2x3el2u2VmBVDCwLHbvk7Sy5IejYjPlu6LiJAUy/1eROyJiHZEtFutVqlhAQxvoNhtX63F0F+IiFeKzWdsryv2r5O0UM+IAKrQ99CbbUt6TtIHEfHUkl37Je2UtLs4fa2WCVHKzMxM0yNgTAxynP1WSQ9IOmb7SLHtCS1G/lfbD0r6WNJ9tUwIoBJ9Y4+ItyR5hd13VDsOgLrwcVkgCWIHkiB2IAliB5IgdiAJvuJ6Bej1NdW6v8I6Ozvbc3+/JaExOjyyA0kQO5AEsQNJEDuQBLEDSRA7kASxA0lwnD25Xbt29dw/PT09mkFQOx7ZgSSIHUiC2IEkiB1IgtiBJIgdSILYgSQ4zn4F6PWd8cXFegAe2YE0iB1IgtiBJIgdSILYgSSIHUiC2IEk+sZue4PtWdvv2z5h+1fF9mnbp20fKX621T8ugGEN8qGaC5Iei4j3bF8v6V3bB4t9T0fE7+obD0BVBlmffV7SfHH+vO0PJK2vezAA1bqs1+y2JyTdLOntYtMjto/a3mt71Qq/M2W7Y7vT7XbLTQtgaAPHbvs6SS9LejQiPpP0jKSbJG3W4iP/k8v9XkTsiYh2RLRbrVb5iQEMZaDYbV+txdBfiIhXJCkizkTElxHxlaRnJW2pb0wAZQ3ybrwlPSfpg4h4asn2dUuudq+k49WPB6Aqg7wbf6ukByQds32k2PaEpB22N0sKSXOSHqphPgAVGeTd+LckeZldB6ofB0Bd+AQdkASxA0kQO5AEsQNJEDuQBLEDSRA7kASxA0kQO5AEsQNJEDuQBLEDSRA7kASxA0l4lEv62u5K+njJpjWSzo5sgMszrrON61wSsw2rytl+GBHL/vtvI439WzdudyKi3dgAPYzrbOM6l8RswxrVbDyNB5IgdiCJpmPf0/Dt9zKus43rXBKzDWskszX6mh3A6DT9yA5gRIgdSKKR2G1vtf1v2x/afryJGVZie872sWIZ6k7Ds+y1vWD7+JJtq20ftH2yOF12jb2GZhuLZbx7LDPe6H3X9PLnI3/NbvsqSf+RdKekU5LekbQjIt4f6SArsD0nqR0RjX8Aw/btkj6X9KeI+Emx7beSzkXE7uJ/lKsi4tdjMtu0pM+bXsa7WK1o3dJlxiXdI+mXavC+6zHXfRrB/dbEI/sWSR9GxEcR8YWklyRtb2COsRcRb0o6d8nm7ZL2Fef3afE/lpFbYbaxEBHzEfFecf68pIvLjDd63/WYaySaiH29pE+WXD6l8VrvPSS9bvtd21NND7OMtRExX5z/VNLaJodZRt9lvEfpkmXGx+a+G2b587J4g+7bbouIWyTdLenh4unqWIrF12DjdOx0oGW8R2WZZca/1uR9N+zy52U1EftpSRuWXL6h2DYWIuJ0cbog6VWN31LUZy6uoFucLjQ8z9fGaRnv5ZYZ1xjcd00uf95E7O9I2mj7RtvXSLpf0v4G5vgW29cWb5zI9rWS7tL4LUW9X9LO4vxOSa81OMs3jMsy3istM66G77vGlz+PiJH/SNqmxXfk/yvpN03MsMJcP5L0r+LnRNOzSXpRi0/r/qfF9zYelPQDSYcknZT0D0mrx2i2P0s6JumoFsNa19Bst2nxKfpRSUeKn21N33c95hrJ/cbHZYEkeIMOSILYgSSIHUiC2IEkiB1IgtiBJIgdSOL/nj2BUNySq8sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample=test_images[14]\n",
    "plt.imshow(sample,cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07dae70b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (28, 28, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-ced91e50484a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/computer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/computer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/computer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/computer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[0;32m--> 646\u001b[0;31m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/computer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m~/anaconda3/envs/computer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/computer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    571\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    574\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (28, 28, 1)"
     ]
    }
   ],
   "source": [
    "predict = model.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "296f45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(sample.reshape(1, 28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f88cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample,cmap=plt.cm.binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01b4900e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efcd6d143c8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow( predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4aadaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mnists.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "514bcdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab93e9c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras.datasets.mnist' has no attribute 'h5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-f97a64644bad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras.datasets.mnist' has no attribute 'h5'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "369c2f3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnists' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-95b3cea15528>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mnists' is not defined"
     ]
    }
   ],
   "source": [
    "load_model(mnists.h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1083cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_data = test_images[3]\n",
    "Input_data = Input_data.reshape(1, 28,28,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "335ecabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = model.save(\"Handwritten.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9862ad11",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Handwritten' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-6f8039a988f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_hand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHandwritten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Handwritten' is not defined"
     ]
    }
   ],
   "source": [
    "model_hand = load_model(Handwritten.h5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c515292",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-e95cb7520e02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodeled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mnists.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/computer/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    144\u001b[0m   if (h5py is not None and (\n\u001b[1;32m    145\u001b[0m       isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/computer/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     model = model_config_lib.model_from_config(model_config,\n\u001b[1;32m    168\u001b[0m                                                custom_objects=custom_objects)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "modeled = load_model(\"mnists.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6ccec38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/taiwo/anaconda3/envs/computer/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: mnist/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "990aba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded =load_model('mnist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "58011d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_data = test_images[7]\n",
    "Input_data = Input_data.reshape(1, 28,28,1)\n",
    "pred = loaded.predict_classes(Input_data, 1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d1783e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0834770f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n",
      "<class 'numpy.ndarray'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(pred)\n",
    "print(type(pred))\n",
    "print(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aa116318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efcd64806a0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample=test_images[3]\n",
    "plt.imshow(sample,cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e322d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
